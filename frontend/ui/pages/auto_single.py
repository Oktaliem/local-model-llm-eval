"""Auto Single Response Grading UI page"""
import streamlit as st
from core.services.evaluation_service import EvaluationService
from typing import List


def render_auto_single_page(evaluation_service: EvaluationService, available_models: List[str]):
    """Render the Auto Single Response Grading page"""
    st.header("Auto Single Response Grading")
    st.markdown("Automatically generate a response from a local model and have an LLM judge evaluate it.")
    
    question = st.text_area(
        "Question/Task:",
        height=100,
        placeholder="Explain the concept of machine learning in simple terms.",
        key="auto_single_question"
    )
    
    st.subheader("Response Generator Model")
    model = st.selectbox(
        "Select Model:",
        available_models,
        index=0,
        key="auto_single_model",
        help="This model will generate the response that will be evaluated by the judge."
    )
    st.info(f"Selected model ({model}) will generate the response, then the judge will evaluate it.")
    
    criteria = st.text_area(
        "Custom Evaluation Criteria (optional):",
        height=100,
        placeholder="Focus on clarity, use of examples, and accessibility to non-technical audiences.",
        key="auto_single_criteria"
    )
    
    col_btn1, col_btn2, col_btn3 = st.columns([2, 1, 1])
    
    with col_btn1:
        evaluate_btn = st.button("üìä Generate & Evaluate", type="primary", use_container_width=True, key="auto_single_evaluate_btn")
    
    with col_btn2:
        save_auto_single_enabled = st.checkbox("üíæ Save to DB", value=True, key="save_auto_single")
    
    with col_btn3:
        if st.button("üîÑ New Evaluation", key="auto_single_new_eval_top", use_container_width=True):
            try:
                st.session_state["auto_single_question"] = ""
                st.session_state["auto_single_criteria"] = ""
                st.session_state["save_auto_single"] = True
            except Exception:
                pass
            st.rerun()
    
    if evaluate_btn:
        if not question:
            st.warning("Please enter a question.")
        else:
            # Import function from core services
            from core.services.llm_service import generate_response
            
            # Get judge model from session state
            judge_model = st.session_state.get("judge_model", "llama3")
            
            # Step 1: Generate Response using the selected model
            with st.status(f"ü§ñ Generating response using {model}...", expanded=True) as status_gen:
                st.write("Sending request to model...")
                result_gen = generate_response(question, model)
                if not result_gen["success"]:
                    status_gen.update(label=f"‚ùå Error generating response", state="error")
                    st.error(f"‚ùå Error generating response: {result_gen['error']}")
                    st.stop()
                generated_response = result_gen["response"]
                status_gen.update(label=f"‚úÖ Response generated", state="complete")
            
            # Display generated response
            st.success("‚úÖ Response Generated!")
            st.markdown(f"### üìù Generated Response (from {model})")
            st.text_area(
                "Response:",
                value=generated_response,
                height=300,
                key="auto_single_generated_response",
                disabled=True,
                help="Full response generated by the selected model. Scroll to see all content."
            )
            
            # Step 2: Evaluate the generated response using the judge
            with st.status(f"üìä Evaluating response using {judge_model}...", expanded=True) as status_eval:
                st.write("Sending evaluation request to judge...")
                result = evaluation_service.evaluate(
                    evaluation_type="single",
                    question=question,
                    judge_model=judge_model,
                    response=generated_response,
                    options={"criteria": criteria if criteria else None},
                    save_to_db=False,  # We handle saving explicitly below
                )
                
                if result.get("success"):
                    st.write("‚úÖ Evaluation received!")
                    status_eval.update(label="‚úÖ Evaluation Complete!", state="complete")
                else:
                    status_eval.update(label="‚ùå Error during evaluation", state="error")
            
            if result.get("success"):
                st.success("‚úÖ Evaluation Complete!")
                st.markdown("### üìã Evaluation Results")
                execution_time = result.get("execution_time", 0)
                if execution_time > 0:
                    st.caption(f"‚è±Ô∏è Execution Time: {execution_time:.2f}s")
                
                judgment_text = result.get("judgment", "")
                if judgment_text and judgment_text.strip():
                    # Use expander for better readability of long judgments
                    with st.expander("üìÑ View Full Evaluation", expanded=True):
                        st.markdown(judgment_text)
                else:
                    st.warning("‚ö†Ô∏è Evaluation content is empty. The model may not have generated a response.")
                
                # Extract and display score if available
                score = result.get("score")
                if score is not None:
                    st.metric("Score", f"{score:.1f}/10")
                
                # Save to database if enabled
                if save_auto_single_enabled:
                    try:
                        from core.services.judgment_service import save_judgment
                        import json
                        
                        metrics = {}
                        if score is not None:
                            metrics["overall_score"] = score
                        metrics_json = json.dumps(metrics) if metrics else None
                        
                        judgment_id = save_judgment(
                            question=question,
                            response_a=generated_response,
                            response_b="",  # Empty for single evaluation
                            model_a=model,
                            model_b="",  # Empty for single evaluation
                            judge_model=judge_model,
                            judgment=judgment_text,
                            judgment_type="single_auto",
                            evaluation_id=result.get("evaluation_id"),
                            metrics_json=metrics_json,
                            trace_json=None,
                        )
                        st.success(f"üíæ Saved to database (ID: {judgment_id})")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Could not save to database: {str(e)}")
                
                # New Evaluation button to reset inputs
                if st.button("üîÑ New Evaluation", key="auto_single_new_evaluation_btn"):
                    try:
                        st.session_state["auto_single_question"] = ""
                        st.session_state["auto_single_criteria"] = ""
                        st.session_state["save_auto_single"] = True
                    except Exception:
                        pass
                    st.rerun()
            else:
                st.error(f"‚ùå Error: {result.get('error', 'Unknown error')}")
                st.info("üí° **Tip:** If the operation is taking too long, you can refresh the page to stop it.")
