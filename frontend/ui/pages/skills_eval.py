"""Skills Evaluation UI page"""
import streamlit as st
import json
from core.services.evaluation_service import EvaluationService
from typing import Optional, List, Dict, Any

def render_skills_eval_page(evaluation_service: EvaluationService):
    """Render the Skills Evaluation page"""
    # Import helper functions from backend services
    from backend.services.data_service import (
        save_skills_evaluation,
        get_skills_evaluations,
        save_judgment
    )
    # TODO: evaluate_skill is a complex wrapper - refactor to use EvaluationService directly
    from backend.services.evaluation_functions import evaluate_skill  # type: ignore
    
    st.header("üéì Skills Evaluation")
    st.markdown("Evaluate domain-specific skills: mathematics, coding, reasoning, and more.")
    
    eval_mode = st.radio(
        "Evaluation Mode:",
        ["Evaluate Skill", "View All Skills Evaluations"],
        horizontal=True,
        key="skills_eval_mode"
    )
    
    if eval_mode == "Evaluate Skill":
        st.markdown("### üìù Evaluate Skill-Specific Response")
        
        # Skill Type Selection
        skill_type = st.selectbox(
            "Skill Type *",
            ["mathematics", "coding", "reasoning", "general"],
            help="Select the type of skill to evaluate",
            key="skill_type"
        )
        
        # Manual/Automated Mode Selection
        response_mode = st.selectbox(
            "Response Mode *",
            ["manual", "automated"],
            help="Manual: Provide your own response. Automated: Let a model generate the response.",
            key="skill_response_mode"
        )
        
        # Domain (optional)
        domain = st.text_input(
            "Domain (optional)",
            placeholder="e.g., algebra, python, logical_puzzles",
            key="skill_domain"
        )
        
        # Question
        question = st.text_area(
            "Question/Task *",
            height=100,
            placeholder="Enter the question or task...",
            key="skill_question"
        )
        
        # Model Selection (only shown in automated mode)
        response_model = None
        if response_mode == "automated":
            # Get available models for response generation
            from backend.services.model_service import get_available_models
            available_models = get_available_models()
            if not available_models:
                available_models = st.session_state.get('available_models', [])
            
            if not available_models:
                st.warning("‚ö†Ô∏è No models available for response generation. Please ensure Ollama is running.")
                st.stop()
            
            response_model = st.selectbox(
                "Select Model *",
                available_models,
                index=0,
                key="skill_response_model",
                help="Model that will generate the response to the question/task"
            )
        
        # Response field - editable in manual mode, read-only in automated mode
        if response_mode == "automated":
            # Show generated response if available from previous evaluation
            response_value = st.session_state.get('skill_generated_response', "")
            
            # Display read-only response field (don't try to update it dynamically)
            if response_value:
                st.text_area(
                    "Response *",
                    height=200,
                    value=response_value,
                    key="skill_response_auto_display",
                    disabled=True,
                    help="Response generated by the selected model"
                )
            else:
                st.text_area(
                    "Response *",
                    height=200,
                    value="",
                    placeholder="Response will be generated automatically by the selected model when you click 'Evaluate Skill'...",
                    key="skill_response_auto_placeholder",
                    disabled=True,
                    help="This field will be populated automatically when you click 'Evaluate Skill'"
                )
            
            # Use stored generated response for evaluation (not from widget)
            response = response_value if response_value else ""
        else:
            # Clear generated response when switching to manual mode
            if 'skill_generated_response' in st.session_state:
                del st.session_state.skill_generated_response
            response = st.text_area(
                "Response *",
                height=200,
                placeholder="Enter the response to evaluate...",
                key="skill_response"
            )
        
        # Reference Answer (optional)
        reference_answer = st.text_area(
            "Reference Answer (optional)",
            height=100,
            placeholder="Enter the expected/correct answer for comparison...",
            key="skill_reference_answer"
        )
        
        # Use judge model from sidebar (consistent with other evaluation pages)
        judge_model = st.session_state.get("judge_model", "gpt-oss-safeguard:20b")
        
        # Display which judge model will be used (read-only info)
        st.info(f"‚öñÔ∏è **Judge Model:** {judge_model} (set in sidebar)")
        
        # Save to DB option
        save_enabled = st.checkbox("üíæ Save to Database", value=True, key="save_skills")
        
        # Evaluate button - use synchronous approach with spinner instead of threading/polling
        if st.button("‚öñÔ∏è Evaluate Skill", type="primary", use_container_width=True):
            if not question:
                st.error("Please provide a question/task")
            elif response_mode == "automated" and not response_model:
                st.error("Please select a model to generate the response")
            elif response_mode == "manual" and not response:
                st.error("Please provide a response to evaluate")
            elif not judge_model:
                st.error("Please select a judge model in the sidebar")
            else:
                # Step 1: Generate response if automated mode
                generated_response = response
                if response_mode == "automated":
                    from core.services.llm_service import generate_response
                    
                    with st.status(f"ü§ñ Generating response using {response_model}...", expanded=True) as status_gen:
                        st.write("Sending request to model...")
                        gen_result = generate_response(question, response_model)
                        if not gen_result["success"]:
                            status_gen.update(label=f"‚ùå Error generating response", state="error")
                            st.error(f"‚ùå Error generating response: {gen_result['error']}")
                            st.stop()
                        generated_response = gen_result["response"]
                        status_gen.update(label=f"‚úÖ Response generated ({len(generated_response)} chars)", state="complete")
                    
                    # Store generated response in session state (separate key, not tied to widget)
                    st.session_state.skill_generated_response = generated_response
                    # Use the generated response for evaluation
                    response = generated_response
                
                # Step 2: Evaluate the response (manual or automated)
                # Use the response variable which is either manual input or generated response
                with st.status(f"‚öñÔ∏è Evaluating skill using {judge_model}...", expanded=True) as status_eval:
                    if response_mode == "automated":
                        status_eval.write(f"üìù Evaluating response generated by {response_model}...")
                    else:
                        status_eval.write("üìù Evaluating provided response...")
                    
                    try:
                        result = evaluate_skill(
                            skill_type=skill_type,
                            question=question,
                            response=response,  # Use generated response if automated, manual response otherwise
                            reference_answer=reference_answer if reference_answer else None,
                            domain=domain if domain else None,
                            judge_model=judge_model
                        )
                        status_eval.update(label="‚úÖ Evaluation complete", state="complete")
                    except Exception as e:
                        status_eval.update(label="‚ùå Evaluation failed", state="error")
                        result = {"success": False, "error": str(e)}
                
                # Store result in session state for display
                st.session_state.skills_eval_result = result
                # Also store the response used for evaluation
                st.session_state.skills_eval_response_used = response
                
                # Show generated response if automated mode (before rerun)
                if response_mode == "automated" and response:
                    st.info(f"üìù **Response generated by {response_model}:**")
                    with st.expander("üìÑ View Generated Response", expanded=True):
                        st.text_area(
                            "Generated Response:",
                            value=response,
                            height=200,
                            disabled=True,
                            key="skill_generated_response_display"
                        )
                
                # Rerun to update the UI with generated response displayed in the read-only field
                st.rerun()
        
        # Display results if available
        if 'skills_eval_result' in st.session_state and st.session_state.skills_eval_result is not None:
            result = st.session_state.skills_eval_result
            
            if result.get("success"):
                st.success("‚úÖ Skills evaluation completed!")
                
                # Display metrics
                st.markdown("### üìä Evaluation Results")
                execution_time = result.get("execution_time", 0)
                if execution_time > 0:
                    st.caption(f"‚è±Ô∏è Execution Time: {execution_time:.2f}s")
                
                col1, col2, col3, col4, col5 = st.columns(5)
                
                with col1:
                    st.metric("Correctness", f"{result['correctness_score']:.2f}/10")
                with col2:
                    st.metric("Completeness", f"{result['completeness_score']:.2f}/10")
                with col3:
                    st.metric("Clarity", f"{result['clarity_score']:.2f}/10")
                with col4:
                    st.metric("Proficiency", f"{result['proficiency_score']:.2f}/10")
                with col5:
                    st.metric("Overall Score", f"{result['overall_score']:.2f}/10")
                
                # Display judgment
                st.markdown("### üìù Detailed Judgment")
                st.markdown(result['judgment_text'])
                
                # Save to database if enabled
                if save_enabled:
                    try:
                        # Use the response that was actually evaluated
                        response_used = st.session_state.get('skills_eval_response_used', response)
                        
                        # Get judge model from session state (set in sidebar)
                        judge_model = st.session_state.get("judge_model", "gpt-oss-safeguard:20b")
                        
                        # Save to skills_evaluations table (detailed table)
                        save_skills_evaluation(
                            skill_type=skill_type,
                            question=question,
                            response=response_used,
                            correctness_score=result['correctness_score'],
                            completeness_score=result['completeness_score'],
                            clarity_score=result['clarity_score'],
                            proficiency_score=result['proficiency_score'],
                            overall_score=result['overall_score'],
                            judgment_text=result['judgment_text'],
                            skill_metrics_json=json.dumps(result['metrics']),
                            trace_json=json.dumps(result['trace']),
                            evaluation_id=result['evaluation_id'],
                            reference_answer=reference_answer if reference_answer else None,
                            domain=domain if domain else None
                        )
                        
                        # Also save to judgments table so it appears in Saved Judgments & Dashboard
                        judgment_text = f"Skills Evaluation ({skill_type}) - Overall Score: {result['overall_score']:.2f}/10 | Correctness: {result['correctness_score']:.2f} | Completeness: {result['completeness_score']:.2f} | Clarity: {result['clarity_score']:.2f} | Proficiency: {result['proficiency_score']:.2f}"
                        save_judgment(
                            question=question,
                            response_a=response_used,
                            response_b="",
                            model_a="Skills Evaluation",
                            model_b="",
                            judge_model=judge_model,
                            judgment=judgment_text,
                            judgment_type="skills_evaluation",
                            evaluation_id=result['evaluation_id'],
                            metrics_json=json.dumps(result['metrics']),
                            trace_json=json.dumps(result['trace'])
                        )
                        
                        st.success(f"üíæ Evaluation saved to database! (ID: {result['evaluation_id']})")
                    except Exception as e:
                        st.warning(f"Evaluation completed but failed to save: {str(e)}")
                
                # Show trace
                with st.expander("üîç View Evaluation Trace"):
                    st.json(result['trace'])
                
                if st.button("üîÑ New Evaluation", key="new_skills_eval"):
                    st.session_state.skills_eval_result = None
                    st.rerun()
            else:
                st.error(f"‚ùå Error: {result.get('error', 'Unknown error')}")
                if st.button("üîÑ Retry", key="retry_skills_eval"):
                    st.session_state.skills_eval_result = None
                    st.rerun()
    
    else:  # View All Skills Evaluations
        st.markdown("### üìã All Skills Evaluations")
        
        # Filter by skill type
        skill_types = ["All", "mathematics", "coding", "reasoning", "general"]
        filter_skill = st.selectbox("Filter by Skill Type", skill_types, key="skills_filter_type")
        
        skill_filter = filter_skill if filter_skill != "All" else None
        evaluations = get_skills_evaluations(limit=100, skill_type=skill_filter)
        
        if not evaluations:
            st.info("No skills evaluations found. Create some evaluations first!")
        else:
            st.success(f"Found {len(evaluations)} skills evaluation(s)")
            
            for eval_item in evaluations:
                with st.expander(f"üéì {eval_item.get('skill_type', 'N/A').upper()} - {eval_item.get('question', 'N/A')[:50]}... - {eval_item.get('created_at', '')}", expanded=False):
                    col_info, col_metrics = st.columns([2, 1])
                    
                    with col_info:
                        st.write(f"**Skill Type:** {eval_item.get('skill_type', 'N/A')}")
                        if eval_item.get('domain'):
                            st.write(f"**Domain:** {eval_item.get('domain', 'N/A')}")
                        st.write(f"**Question:** {eval_item.get('question', 'N/A')}")
                        st.write(f"**Response:** {eval_item.get('response', 'N/A')[:200]}...")
                        if eval_item.get('reference_answer'):
                            st.write(f"**Reference Answer:** {eval_item.get('reference_answer', 'N/A')}")
                    
                    with col_metrics:
                        st.metric("Correctness", f"{eval_item.get('correctness_score', 0):.2f}/10")
                        st.metric("Completeness", f"{eval_item.get('completeness_score', 0):.2f}/10")
                        st.metric("Clarity", f"{eval_item.get('clarity_score', 0):.2f}/10")
                        st.metric("Proficiency", f"{eval_item.get('proficiency_score', 0):.2f}/10")
                        st.metric("Overall Score", f"{eval_item.get('overall_score', 0):.2f}/10")
                    
                    st.markdown("---")
                    st.markdown("**Judgment:**")
                    st.markdown(eval_item.get('judgment_text', 'N/A'))
                    
                    # Show metrics if available
                    if eval_item.get('skill_metrics_json'):
                        with st.expander("View Detailed Metrics"):
                            try:
                                metrics = json.loads(eval_item.get('skill_metrics_json', '{}'))
                                st.json(metrics)
                            except:
                                pass
                    
                    # Show trace if available
                    if eval_item.get('trace_json'):
                        with st.expander("View Evaluation Trace"):
                            try:
                                trace = json.loads(eval_item.get('trace_json', '{}'))
                                st.json(trace)
                            except:
                                pass

